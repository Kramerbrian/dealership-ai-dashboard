# âœ… Redis Idempotency Integration - Bulk CSV Upload

## ğŸ¯ What Was Added

### 1. **File-Level Idempotency** (`/api/origins/bulk-csv`)
- âœ… Checks Redis for duplicate file uploads using SHA256 checksum
- âœ… Prevents same file from being uploaded multiple times
- âœ… Caches upload metadata after successful preview
- âœ… Allows re-upload after 24 hours or from different tenant
- âœ… Returns 409 Conflict with existing upload details if duplicate

### 2. **Commit-Level Idempotency** (`/api/origins/bulk-csv/commit`)
- âœ… Checks Redis for duplicate commit requests using idempotency key
- âœ… Returns cached results for duplicate commits (true idempotency)
- âœ… Caches successful commits for 48 hours
- âœ… Prevents duplicate writes to Fleet API

## ğŸ”‘ Redis Key Patterns

### File Checksum Cache:
```
bulk:checksum:<SHA256_FILE_CHECKSUM>
```

**Value Structure:**
```json
{
  "uploadId": "abc123...",
  "timestamp": 1234567890,
  "tenantId": "tenant-001",
  "rowsCount": 42
}
```

**TTL:** 24 hours (86400 seconds)

### Commit Idempotency Cache:
```
bulk:commit:<IDEMPOTENCY_KEY>
```

**Value Structure:**
```json
{
  "results": [...],
  "timestamp": 1234567890
}
```

**TTL:** 48 hours (172800 seconds)

## ğŸ”„ Flow Diagram

```
1. Upload CSV
   â†“
2. Compute SHA256 checksum
   â†“
3. Check Redis: bulk:checksum:<checksum>
   â†“
4a. If exists & same tenant & < 24h â†’ Return 409 Conflict
4b. If not exists or expired â†’ Proceed with parsing
   â†“
5. Parse & validate CSV
   â†“
6. Cache checksum in Redis (24h TTL)
   â†“
7. Return preview with uploadId
   â†“
8. User clicks "Commit"
   â†“
9. Generate idempotency key from row checksums
   â†“
10. Check Redis: bulk:commit:<idempotencyKey>
    â†“
11a. If exists â†’ Return cached results (idempotent)
11b. If not exists â†’ Proceed with Fleet API call
    â†“
12. Call Fleet API
    â†“
13. Cache results in Redis (48h TTL)
    â†“
14. Return results
```

## ğŸ§ª Testing

### Test Duplicate Upload Detection:

```bash
# 1. Upload CSV file
curl -X POST http://localhost:3000/api/origins/bulk-csv \
  -F "file=@test.csv" \
  -H "Authorization: Bearer $TOKEN"

# 2. Check Redis (in Upstash CLI):
KEYS bulk:checksum:*
GET bulk:checksum:<checksum>

# 3. Try uploading same file again
# Should return 409 Conflict
```

### Test Commit Idempotency:

```bash
# 1. Commit upload
curl -X POST http://localhost:3000/api/origins/bulk-csv/commit \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer $TOKEN" \
  -d '{"rows": [...]}'

# 2. Check Redis:
KEYS bulk:commit:*
GET bulk:commit:<idempotencyKey>

# 3. Retry same commit (should return cached result)
# Should return same results with cached: true
```

## ğŸ“Š Monitoring Keys

### View All Bulk Upload Keys:
```redis
KEYS bulk:*
```

### Count by Type:
```redis
EVAL "return #redis.call('keys', 'bulk:checksum:*')" 0
EVAL "return #redis.call('keys', 'bulk:commit:*')" 0
```

### Check TTL:
```redis
TTL bulk:checksum:<checksum>
TTL bulk:commit:<idempotencyKey>
```

### View Cached Data:
```redis
GET bulk:checksum:<checksum>
GET bulk:commit:<idempotencyKey>
```

## ğŸ”§ Configuration

### TTL Values:
- **File Checksum:** 24 hours (86400 seconds)
  - Prevents duplicate uploads within 24h from same tenant
  - Allows re-upload after expiration or from different tenant

- **Commit Cache:** 48 hours (172800 seconds)
  - Longer TTL for commit results (write operations)
  - Allows retries without duplicate API calls

### Graceful Degradation:
- If Redis is unavailable, the routes continue to work
- Only idempotency protection is lost (no duplicate detection)
- All other functionality remains intact

## âœ… Benefits

1. **Prevents Duplicate Uploads:** Same file can't be uploaded twice within 24h
2. **Idempotent Commits:** Retry-safe commit operations
3. **Performance:** Cached results reduce Fleet API calls
4. **User Experience:** Clear error messages for duplicate uploads
5. **Cost Savings:** Reduces unnecessary API calls and processing

## ğŸš€ Production Checklist

- [x] Redis idempotency checks added
- [x] File checksum caching (24h TTL)
- [x] Commit result caching (48h TTL)
- [x] Graceful degradation (works without Redis)
- [x] Clear error messages for duplicates
- [x] Cache key patterns documented
- [x] Testing commands provided

## ğŸ“ Notes

- Checksums use SHA256 for files, SHA1 for rows
- Different tenants can upload same file (different cache keys)
- Expired cache entries allow re-upload (24h window)
- Commit cache prevents duplicate Fleet API calls on retries

